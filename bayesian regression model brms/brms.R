 library(brms) # Bayesian regression model
 library(bayesplot)
 library(ggplot2)
 #Data
 qplot(x=Girth, y=Height, data=trees)


 # Our usual approach (Linear Regression Model)

 fit.f <- lm(Height ~ Girth, data= trees)
 layout(matrix(1:4, nrow=2))
 plot(fit.f)
 summary(fit.f)
 confint(fit.f)


 # A better approach : Bayesian Regression Model

 fit.b <- brm(Height ~ Girth, data= trees)  # Getting posterior distribution fit.b with Using uniform prior Uninformed belief)

 summary(fit.b)


 plot(fit.b, variable = c("b_Girth"))

 #Caterpillar plot of chains are actually evident that as the simulation (chains) is running stan is sampling from all over the distribution; u can see here mean is centered around 1 and stan is sampling above it and below it with a frequency that is reflected by this probability distribution (posterior having mean 1.0 unlike of prior having mean of 1.5); so this simulation through time it runs it and hold things constant it bounces around this landscape of prob distribution kind of looking for a hill by checking whole landscape and it tends to sample one more than other thing and that whats those chains are

 plot(conditional_effects(fit.b))

 newdata <- data.frame(Girth = c(30))

 newdata

 #Predict future observations
 predict(fit.b, newdata=  newdata)

 # Get specific estimate
 fitted(fit.b, newdata= newdata)

 # Assume that you have the prior belief about the girth
 prior_summary(fit.b)


 #old and new priors on slope

 #old

 curve(dnorm(x, m=1.5,sd=0.5), -1,5,
 ylab= "Prior probability on slope",
 xlab="Slope",
 main="Possible Current Understanding")
 curve(dnorm(x, m=0,sd=100),-100,100, add=TRUE, lty=2,lwd=2)
 curve(dunif(x, min=0,max=5),0, 5, add=TRUE, lty=3, lwd=2, col=2)
 curve(dgamma(x, scale=1, shape=2), 0, 5, add= TRUE, lyt=3, lwd=2, col=2)




 slope.prior <- c(prior(normal(1.5, 0.5), class= b, coef= Girth)) # Using normal prior

 fit.b2<- brm(Height ~ Girth, data = trees, # Posterior distribution fit.b2 generating by taking into account my past literature informed prior which here is normal prior with some mean and sd = normal(1.5, 0.5)
              # So kowing some prior belief having knowledge of relation between the height and girth of trees as normal(1.5, 0.5)
              prior = slope.prior)
 # And bringing that new knowledge into my analysis I say what more we can learn and that what more we learn is that fit.b2 posterior distribution generated by stan using MCMC
 prior_summary(fit.b2)


 # Check MCMC Chains
 plot(fit.b2)

 # Check simple diagnostics
 summary(fit.b)

 summary(fit.b2)

 #This Rhat shows how different these chains are,,Rhat is measure of how variable each one is compared to how different the chains are it is kind of  analysis of variance , comparing withing and among variance of those chains

 # Posterior Predictive check

 # Do simulated mean (blue) match our observed mean(black) ?

 # If our model is good model, we can use it to simulate something that look like our data
 # In particular, Bayesian learning occurs when the posterior distributions can differ from the prior distributions, reflecting that we have updated our beliefs based on the current data. Potentially, one can choose informative prior distributions for the parameters in a model that is underidentified from a frequentist perspective, and still obtain Bayesian identifiability for unknowns of interest.
 #we run bayesplot
 # If we use the model we made to simulate more data, then we might come up with different means in our simulated data


 bayesplot_grid(
     pp_check(fit.b,  type= 'stat', stat= 'mean'),
     pp_check(fit.b2,  type= 'stat', stat= 'mean')

 )

 # Above are the simulated means

 # Look at an estimate , below are the underlying distributions

 mcmc_plot(fit.b)
 mcmc_plot(fit.b, variable= "Girth")


# Compare estimates
 #Samples from the posterior distributions fit.b and fit.b2

 samples <- as.data.frame(fit.b) #Taking sample (Simulating) from distribution fit.b that gives me df
 samples2<- as.data.frame(fit.b2) #Taking sample (Simulating) from distribution fit.b2 that gives me df

 samples # These samples have these columns b_Intercept :   b_Girth    sigma    lprior       lp__ (it is likelihood)
 samples2

 ggplot(data=samples, aes(x=b_Girth)) + geom_density() +
            geom_density(data= samples2, aes(x=b_Girth), color="red", linetype=2) +
            labs(title= "Our beliefs about the slope")


  # Now asking different Questions from those posteriors

 # What is the probability that the slope > 1.1 ?

 #(i.e, what proportion of the samples are > 1.1 ?)


 n <- length(samples$b_Girth)
 sum(samples$b_Girth > 1.1)/n


 # Or test whether samples$b_Girth -1.1 > 0


 # With probability of 0.90

 test_samples <- samples$b_Girth -1.1

 # Simple quantiles
 quantile(test_samples, c(0.05, .5, 0.95))


 # Highest probabilit density interval

 HPDinterval(as.mcmc(test_samples), prob=0.90)

 #in brms

 hypothesis(fit.b2, "Girth>1.1")










